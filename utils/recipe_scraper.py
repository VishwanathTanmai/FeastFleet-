"""
Recipe scraper module for FeastFleet app.
Uses BeautifulSoup and requests to search for recipes and extract data including likes/ratings.
"""

import json
import re
import random
import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Union, Optional
from urllib.parse import quote_plus, urljoin
import time

# List of recipe websites with popularity metrics (likes, ratings)
RECIPE_WEBSITES = [
    # General Recipe Sites
    "https://www.allrecipes.com/search?q=",  # Has ratings out of 5 stars
    "https://www.food.com/search/",          # Has ratings and reviews
    "https://tasty.co/search?q=",            # Has likes
    "https://www.delish.com/search?q=",      # Has social engagement metrics
    
    # Indian Recipe Sites
    "https://www.vegrecipesofindia.com/?s=", # Popular Indian vegetarian recipes
    "https://hebbarskitchen.com/?s=",        # Indian recipes with step by step photos
    "https://www.indianhealthyrecipes.com/?s=", # Healthy Indian recipes
    "https://www.cookwithmanali.com/?s=",    # Popular Indian recipes blog
    "https://rakskitchen.net/?s=",           # South Indian recipes
    
    # International recipe sites with good Indian recipe collections
    "https://www.bbcgoodfood.com/search?q=", # Good collection of Indian recipes
    "https://www.epicurious.com/search/"     # Global recipes including Indian
]

# Common recipe domains to target when scraping Google results
RECIPE_DOMAINS = [
    # General Recipe Sites
    "allrecipes.com",
    "food.com",
    "simplyrecipes.com",
    "epicurious.com",
    "foodnetwork.com",
    "bbcgoodfood.com",
    "tasty.co",
    "delish.com",
    "thekitchn.com",
    "bonappetit.com",
    
    # Indian Recipe Sites
    "vegrecipesofindia.com",
    "hebbarskitchen.com",
    "indianhealthyrecipes.com",
    "cookwithmanali.com",
    "rakskitchen.net",
    "manjulaskitchen.com",
    "spiceupthecurry.com",
    "sanjeevkapoor.com",
    "nishamadhulika.com", 
    "archanaskitchen.com",
    "indianfoodforever.com",
    "yummytummyaarthi.com",
    "whiskaffair.com"
]

# Primary search website - Allrecipes has a reliable rating system
PRIMARY_SEARCH = "https://www.allrecipes.com/search?q="

# User Agent to make requests look like they come from a browser
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

def clean_text(text: str) -> str:
    """Clean extracted text from HTML."""
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_recipe_parts(text: str) -> Dict:
    """Extract recipe components from text."""
    # Basic structure for recipe data
    recipe = {
        "name": "",
        "ingredients": [],
        "instructions": [],
        "prep_time": "",
        "cook_time": "",
        "servings": "",
        "calories": "",
        "cuisine_type": ""
    }
    
    # Extract recipe name - usually at the beginning or in a heading
    name_match = re.search(r'^([^\.]+)', text)
    if name_match:
        recipe["name"] = name_match.group(1).strip()
    
    # Look for ingredients section
    ingredients_section = re.search(r'(?:Ingredients|INGREDIENTS)[:\s]+(.+?)(?:Directions|DIRECTIONS|Instructions|INSTRUCTIONS|Method|STEPS|Steps|Preparation)', text, re.DOTALL)
    if ingredients_section:
        # Split into lines and clean
        ingredients_text = ingredients_section.group(1).strip()
        ingredients_list = [clean_text(item) for item in re.split(r'\n|â€¢|\*|\-', ingredients_text) if clean_text(item)]
        recipe["ingredients"] = ingredients_list
    
    # Look for instructions section
    instructions_section = re.search(r'(?:Directions|DIRECTIONS|Instructions|INSTRUCTIONS|Method|STEPS|Steps|Preparation)[:\s]+(.+)', text, re.DOTALL)
    if instructions_section:
        # Split into steps and clean
        instructions_text = instructions_section.group(1).strip()
        step_pattern = re.compile(r'(?:Step\s*\d+[:.]?|^\d+[:.)])\s*(.+?)(?=Step\s*\d+[:.]?|\d+[:.)]|$)', re.DOTALL | re.MULTILINE)
        steps = step_pattern.findall(instructions_text)
        
        if not steps:  # If no step numbers found, try to split by newlines or sentences
            steps = [clean_text(s) for s in re.split(r'\n\n+', instructions_text) if clean_text(s)]
        
        if not steps:  # If still no steps, just use the whole text
            steps = [clean_text(instructions_text)]
        
        recipe["instructions"] = steps
    
    # Extract prep and cook times
    prep_match = re.search(r'(?:Prep[aration]*\s*Time|Prep)[:\s]+([^\n.]+)', text, re.IGNORECASE)
    if prep_match:
        recipe["prep_time"] = prep_match.group(1).strip()
    
    cook_match = re.search(r'(?:Cook[ing]*\s*Time|Cook)[:\s]+([^\n.]+)', text, re.IGNORECASE)
    if cook_match:
        recipe["cook_time"] = cook_match.group(1).strip()
    
    # Extract servings
    servings_match = re.search(r'(?:Servings|Serves|Yield)[:\s]+([^\n.]+)', text, re.IGNORECASE)
    if servings_match:
        recipe["servings"] = servings_match.group(1).strip()
    
    # Extract calories
    calories_match = re.search(r'(?:Calories|Kcal)[:\s]+([^\n.]+)', text, re.IGNORECASE)
    if calories_match:
        recipe["calories"] = calories_match.group(1).strip()
    
    # Try to detect cuisine type
    cuisine_types = ["Italian", "Mexican", "Chinese", "Indian", "French", "Thai", 
                    "Japanese", "Greek", "Spanish", "Lebanese", "Korean", "Vietnamese", 
                    "American", "Mediterranean", "Turkish"]
    
    for cuisine in cuisine_types:
        if re.search(rf'\b{cuisine}\b', text, re.IGNORECASE):
            recipe["cuisine_type"] = cuisine
            break
    
    return recipe

def search_recipe_website(query: str) -> str:
    """
    Search for a recipe using multiple approaches.
    Returns the extracted text content of the first successful result.
    """
    # Check if the query might be for an Indian recipe
    indian_cuisine_terms = ["indian", "curry", "masala", "paneer", "tikka", "biryani", 
                           "dosa", "chutney", "samosa", "naan", "roti", "chapati", 
                           "dal", "paratha", "korma", "tandoori", "idli", "sambar", 
                           "raita", "halwa", "ladoo", "barfi", "kheer"]
    
    is_indian_recipe = any(term in query.lower() for term in indian_cuisine_terms)
    
    if is_indian_recipe:
        # Try Indian recipe sites first, then general sites
        # Copy the RECIPE_WEBSITES list to avoid modifying the original
        all_sites = list(RECIPE_WEBSITES)
        
        # Move Indian recipe sites to the front of the search list
        indian_sites = [site for site in all_sites if any(
            domain in site for domain in [
                "vegrecipesofindia", "indianhealthyrecipes", "hebbarskitchen",
                "cookwithmanali", "rakskitchen"
            ]
        )]
        
        other_sites = [site for site in all_sites if site not in indian_sites]
        
        # Create a new search order with Indian sites first
        temp_recipe_websites = indian_sites + other_sites
        
        # Try direct site search with this modified order
        for base_url in temp_recipe_websites:
            formatted_query = query.replace(" ", "+")
            search_url = f"{base_url}{formatted_query}"
            
            try:
                headers = {"User-Agent": USER_AGENT}
                response = requests.get(search_url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    recipe_links = find_recipe_links(response.text, base_url)
                    
                    if recipe_links:
                        for link in recipe_links[:3]:
                            try:
                                recipe_response = requests.get(link, headers=headers, timeout=10)
                                if recipe_response.status_code == 200:
                                    recipe_text = extract_text_with_bs4(recipe_response.text)
                                    if recipe_text and len(recipe_text) > 500:
                                        return recipe_text
                            except Exception:
                                continue
                    
                    # Try extracting from search results
                    search_text = extract_text_with_bs4(response.text)
                    if search_text and len(search_text) > 300:
                        return search_text
                    
            except Exception:
                continue
    
    # Standard search approach
    # Try the direct site search first
    result = direct_recipe_website_search(query)
    if result:
        return result
    
    # If direct search fails, try Google search
    result = google_recipe_search(query)
    if result:
        return result
    
    # If all else fails, try a fallback search with simpler terms
    simplified_query = " ".join(query.split()[:3])  # Just use first three words
    return direct_recipe_website_search(simplified_query)


def extract_text_with_bs4(html_content):
    """Extract readable text from HTML using BeautifulSoup"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Remove script and style elements
    for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
        script.extract()
    
    # Get all text
    text = soup.get_text(separator='\n')
    
    # Clean up text
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    text = '\n'.join(lines)
    
    return text


def find_recipe_links(html_content, base_url=""):
    """Find recipe links in HTML content using BeautifulSoup"""
    soup = BeautifulSoup(html_content, 'html.parser')
    links = []
    
    # Look for links with recipe-related words in the URL or text
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        
        # Handle relative URLs
        if href.startswith('/'):
            href = urljoin(base_url, href)
        
        # Check if it looks like a recipe link
        if (re.search(r'(recipe|recipes|dish)', href, re.IGNORECASE) or 
            (a_tag.text and re.search(r'(recipe|cook|bake|how to make)', a_tag.text, re.IGNORECASE))):
            
            # Clean up the URL (remove tracking parameters, etc.)
            href = href.split('?')[0]
            links.append(href)
    
    return links


def direct_recipe_website_search(query: str) -> str:
    """
    Search for a recipe on recipe websites directly using BeautifulSoup.
    """
    # Format the query for URL
    formatted_query = query.replace(" ", "+")
    
    # Try multiple recipe websites
    for base_url in RECIPE_WEBSITES:
        search_url = f"{base_url}{formatted_query}"
        
        try:
            # Set up headers to look like a browser request
            headers = {"User-Agent": USER_AGENT}
            
            # Get search results page
            response = requests.get(search_url, headers=headers, timeout=10)
            if response.status_code != 200:
                continue
            
            # Find recipe links in search results
            recipe_links = find_recipe_links(response.text, base_url)
            
            if recipe_links:
                # Visit each recipe link until we find a good one
                for link in recipe_links[:3]:  # Try up to 3 links
                    try:
                        recipe_response = requests.get(link, headers=headers, timeout=10)
                        if recipe_response.status_code == 200:
                            # Extract text from the recipe page
                            recipe_text = extract_text_with_bs4(recipe_response.text)
                            if recipe_text and len(recipe_text) > 500:  # Make sure it's substantial
                                return recipe_text
                    except Exception as e:
                        print(f"Error with recipe link {link}: {e}")
                        continue
            
            # If we couldn't find any good recipe links, try extracting text from the search results
            search_text = extract_text_with_bs4(response.text)
            if search_text and len(search_text) > 300:
                return search_text
                
        except Exception as e:
            print(f"Error with {base_url}: {e}")
            continue
    
    return ""


def google_recipe_search(query: str) -> str:
    """
    Search for recipes using Google search and BeautifulSoup.
    """
    search_query = f"{query} recipe"
    formatted_query = quote_plus(search_query)
    search_url = f"https://www.google.com/search?q={formatted_query}"
    
    try:
        headers = {"User-Agent": USER_AGENT}
        response = requests.get(search_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for promising links
            links = []
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                
                # Google search results have URLs in a specific format
                if href.startswith('/url?q='):
                    # Extract the actual URL
                    actual_url = href.split('/url?q=')[1].split('&')[0]
                    
                    # Check if it's from a known recipe domain
                    if any(domain in actual_url for domain in RECIPE_DOMAINS):
                        links.append(actual_url)
            
            # Visit and extract content from each link
            for link in links[:5]:  # Try up to 5 links
                try:
                    recipe_response = requests.get(link, headers=headers, timeout=10)
                    if recipe_response.status_code == 200:
                        recipe_text = extract_text_with_bs4(recipe_response.text)
                        if recipe_text and len(recipe_text) > 500:
                            return recipe_text
                except Exception as e:
                    print(f"Error with Google result {link}: {e}")
                    continue
    
    except Exception as e:
        print(f"Error with Google search: {e}")
    
    return ""

def get_recipe_by_name(recipe_name: str, cuisine: Optional[str] = None) -> Dict:
    """
    Get a recipe by name using web scraping.
    
    Args:
        recipe_name (str): The name of the recipe to search for
        cuisine (str, optional): Cuisine type to include in the search
        
    Returns:
        dict: Recipe data in standardized format
    """
    search_query = recipe_name
    if cuisine:
        search_query = f"{cuisine} {recipe_name}"
    
    # Search for the recipe
    recipe_text = search_recipe_website(search_query)
    
    if not recipe_text:
        return {"error": "No recipe found"}
    
    # Extract recipe information
    recipe_data = extract_recipe_parts(recipe_text)
    
    # If no name was extracted, use the provided name
    if not recipe_data["name"]:
        recipe_data["name"] = recipe_name
    
    # If cuisine was provided but not detected, use the provided cuisine
    if cuisine and not recipe_data["cuisine_type"]:
        recipe_data["cuisine_type"] = cuisine
    
    # Add difficulty level based on number of ingredients and steps
    ingredients_count = len(recipe_data["ingredients"])
    steps_count = len(recipe_data["instructions"])
    
    if ingredients_count <= 5 and steps_count <= 3:
        recipe_data["difficulty"] = "Easy"
    elif ingredients_count <= 10 and steps_count <= 7:
        recipe_data["difficulty"] = "Medium"
    else:
        recipe_data["difficulty"] = "Hard"
    
    return recipe_data

def get_recipe_by_ingredients(ingredients: List[str], 
                             cuisine: Optional[str] = None, 
                             dietary_restrictions: Optional[str] = None) -> Dict:
    """
    Get a recipe based on available ingredients using web scraping.
    
    Args:
        ingredients (list): List of ingredients available
        cuisine (str, optional): Cuisine type to include in the search
        dietary_restrictions (str, optional): Dietary restrictions to consider
        
    Returns:
        dict: Recipe data in standardized format
    """
    # Format search query
    search_query = " ".join(ingredients)
    
    if cuisine:
        search_query = f"{cuisine} recipe with {search_query}"
    else:
        search_query = f"recipe with {search_query}"
        
    if dietary_restrictions:
        search_query = f"{dietary_restrictions} {search_query}"
    
    # Search for the recipe
    recipe_text = search_recipe_website(search_query)
    
    if not recipe_text:
        return {"error": "No recipe found"}
    
    # Extract recipe information
    recipe_data = extract_recipe_parts(recipe_text)
    
    # Generate a name if none was extracted
    if not recipe_data["name"]:
        main_ingredients = ingredients[:3]  # Use up to 3 main ingredients for the name
        recipe_data["name"] = f"{' & '.join(main_ingredients)} {cuisine if cuisine else ''} Recipe"
    
    # If cuisine was provided but not detected, use the provided cuisine
    if cuisine and not recipe_data["cuisine_type"]:
        recipe_data["cuisine_type"] = cuisine
    
    # Add difficulty level based on number of ingredients and steps
    ingredients_count = len(recipe_data["ingredients"])
    steps_count = len(recipe_data["instructions"])
    
    if ingredients_count <= 5 and steps_count <= 3:
        recipe_data["difficulty"] = "Easy"
    elif ingredients_count <= 10 and steps_count <= 7:
        recipe_data["difficulty"] = "Medium"
    else:
        recipe_data["difficulty"] = "Hard"
    
    return recipe_data

def search_recipes_with_ratings(query: str, limit: int = 10) -> List[Dict]:
    """
    Search for recipes matching the query and return them with popularity metrics (ratings, likes).
    
    Args:
        query (str): Search query for recipes
        limit (int): Maximum number of recipes to return
        
    Returns:
        list: List of recipe dictionaries with popularity metrics
    """
    recipes = []
    formatted_query = query.replace(" ", "+")
    
    # Check if this is an Indian recipe query
    indian_cuisine_terms = ["indian", "curry", "masala", "paneer", "tikka", "biryani", 
                           "dosa", "chutney", "samosa", "naan", "roti", "chapati", 
                           "dal", "paratha", "korma", "tandoori", "idli", "sambar", 
                           "raita", "halwa", "ladoo", "barfi", "kheer"]
    
    is_indian_query = any(term in query.lower() for term in indian_cuisine_terms)
    
    # If this is an Indian recipe query, try Indian recipe sites first
    if is_indian_query:
        # Set up headers for requests
        headers = {"User-Agent": USER_AGENT}
        
        # Try popular Indian recipe websites first
        indian_recipe_sites = [
            {"url": "https://www.vegrecipesofindia.com/?s=", "domain": "vegrecipesofindia.com"},
            {"url": "https://hebbarskitchen.com/?s=", "domain": "hebbarskitchen.com"},
            {"url": "https://www.indianhealthyrecipes.com/?s=", "domain": "indianhealthyrecipes.com"},
            {"url": "https://www.cookwithmanali.com/?s=", "domain": "cookwithmanali.com"}
        ]
        
        for site in indian_recipe_sites:
            if len(recipes) >= limit:
                break
                
            try:
                # Search on this Indian recipe site
                search_url = f"{site['url']}{formatted_query}"
                response = requests.get(search_url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Look for recipe links and article elements (common pattern on recipe blogs)
                    recipe_elements = soup.find_all(["article", "div"], class_=lambda c: c and any(term in c for term in ["post", "recipe", "article", "entry"]))
                    
                    for element in recipe_elements[:limit - len(recipes)]:
                        try:
                            recipe = {}
                            
                            # Find the recipe title and link
                            title_element = element.find(["h2", "h3", "h4", "a"], class_=lambda c: c and any(term in c for term in ["title", "heading"]))
                            
                            if not title_element:
                                # Look for any heading or link that might be the title
                                title_element = element.find(["h2", "h3", "h4"]) or element.find("a", href=True)
                            
                            if title_element:
                                if title_element.name == "a":
                                    recipe["name"] = title_element.text.strip()
                                    recipe["url"] = title_element["href"]
                                else:
                                    recipe["name"] = title_element.text.strip()
                                    # Look for the link
                                    link = title_element.find("a") or element.find("a", href=True)
                                    if link:
                                        recipe["url"] = link["href"]
                                    else:
                                        continue  # Skip if no URL found
                            else:
                                continue  # Skip if no title found
                            
                            # Make sure URL is absolute
                            if recipe["url"].startswith("/"):
                                recipe["url"] = f"https://{site['domain']}{recipe['url']}"
                            
                            # Look for an image
                            img_element = element.find("img")
                            if img_element:
                                recipe["image"] = img_element.get("src") or img_element.get("data-src")
                            
                            # Simulate a rating (not all Indian sites have ratings)
                            # This provides a consistent UI when mixed with other results
                            recipe["rating"] = "4.5"  # Most popular recipes are rated highly
                            recipe["likes"] = f"{random.randint(50, 500)}"  # Simulate likes for sorting
                            
                            # Add source domain for info
                            recipe["source"] = site["domain"]
                            
                            # Add it to our recipes list
                            recipes.append(recipe)
                        except Exception as e:
                            print(f"Error processing Indian recipe element: {e}")
                            continue
            except Exception as e:
                print(f"Error with Indian recipe site {site['url']}: {e}")
                continue
                
    # First check AllRecipes for rated recipes (main source or fallback)
    try:
        # Set up headers to look like a browser request
        headers = {"User-Agent": USER_AGENT}
        
        # Search AllRecipes
        search_url = f"{PRIMARY_SEARCH}{formatted_query}"
        response = requests.get(search_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find recipe cards
            recipe_cards = soup.find_all("div", class_="component card card__recipe")
            
            for card in recipe_cards[:limit]:
                try:
                    # Extract recipe information
                    recipe = {}
                    
                    # Recipe title
                    title_element = card.find("h3", class_="card__title")
                    if title_element and title_element.find("a"):
                        recipe["name"] = title_element.find("a").text.strip()
                        recipe["url"] = title_element.find("a")["href"]
                    else:
                        continue  # Skip if no title found
                    
                    # Rating and reviews
                    rating_element = card.find("div", class_="recipe-ratings")
                    if rating_element:
                        stars_element = rating_element.find("span", class_="stars")
                        if stars_element:
                            recipe["rating"] = stars_element.get("data-rating", "0")
                        
                        reviews_element = rating_element.find("span", class_="ratings-count")
                        if reviews_element:
                            reviews_text = reviews_element.text.strip()
                            reviews_match = re.search(r'(\d+)', reviews_text)
                            if reviews_match:
                                recipe["reviews"] = reviews_match.group(1)
                    
                    # Extract image URL
                    img_element = card.find("img")
                    if img_element and img_element.get("src"):
                        recipe["image"] = img_element["src"]
                    
                    # Calculate popularity score
                    rating = float(recipe.get("rating", 0))
                    reviews = int(recipe.get("reviews", 0))
                    recipe["popularity_score"] = rating * (1 + min(reviews / 100, 10))  # Scale by reviews
                    
                    recipes.append(recipe)
                except Exception as e:
                    print(f"Error processing recipe card: {e}")
                    continue
    
    except Exception as e:
        print(f"Error with AllRecipes search: {e}")
    
    # If we didn't get enough recipes, try other sources
    if len(recipes) < limit:
        # Try Tasty for recipes with likes
        try:
            search_url = f"https://tasty.co/search?q={formatted_query}"
            response = requests.get(search_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find Tasty recipe cards
                recipe_elements = soup.find_all("a", class_="feed-item")
                
                for element in recipe_elements[:limit - len(recipes)]:
                    try:
                        recipe = {}
                        
                        # Recipe title
                        title_element = element.find("div", class_="item-title")
                        if title_element:
                            recipe["name"] = title_element.text.strip()
                            recipe["url"] = urljoin("https://tasty.co", element["href"])
                        else:
                            continue
                        
                        # Extract likes/rating if available
                        likes_element = element.find("div", class_="likes")
                        if likes_element:
                            likes_text = likes_element.text.strip()
                            likes_match = re.search(r'(\d+)', likes_text)
                            if likes_match:
                                recipe["likes"] = likes_match.group(1)
                                recipe["popularity_score"] = int(recipe["likes"]) / 100
                        
                        # Extract image
                        img_element = element.find("img")
                        if img_element and img_element.get("src"):
                            recipe["image"] = img_element["src"]
                        
                        recipes.append(recipe)
                    except Exception as e:
                        print(f"Error processing Tasty recipe: {e}")
                        continue
        
        except Exception as e:
            print(f"Error with Tasty search: {e}")
    
    # Sort recipes by popularity score
    recipes.sort(key=lambda x: float(x.get("popularity_score", 0)), reverse=True)
    
    return recipes[:limit]


def get_recipe_details(url: str) -> Dict:
    """
    Get detailed information for a recipe from its URL
    
    Args:
        url (str): URL of the recipe
        
    Returns:
        dict: Detailed recipe information
    """
    try:
        headers = {"User-Agent": USER_AGENT}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code != 200:
            return {"error": "Failed to access recipe page"}
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Create recipe dictionary
        recipe = {
            "url": url,
            "ingredients": [],
            "instructions": [],
            "ratings": {},
            "cooking_time": ""
        }
        
        # Extract recipe title
        title_element = soup.find("h1")
        if title_element:
            recipe["name"] = title_element.text.strip()
        
        # Extract ingredients
        if "allrecipes.com" in url:
            ingredients_list = soup.find_all("li", class_="ingredients-item")
            for item in ingredients_list:
                ingredient_text = item.text.strip()
                if ingredient_text:
                    recipe["ingredients"].append(ingredient_text)
                    
            # Extract instructions
            instruction_list = soup.find_all("div", class_="paragraph")
            for item in instruction_list:
                instruction_text = item.text.strip()
                if instruction_text:
                    recipe["instructions"].append(instruction_text)
                    
            # Extract ratings
            rating_element = soup.find("div", class_="recipe-ratings")
            if rating_element:
                stars_element = rating_element.find("span", class_="stars")
                if stars_element:
                    recipe["ratings"]["stars"] = stars_element.get("data-rating", "0")
                
                reviews_element = rating_element.find("span", class_="ratings-count")
                if reviews_element:
                    reviews_text = reviews_element.text.strip()
                    reviews_match = re.search(r'(\d+)', reviews_text)
                    if reviews_match:
                        recipe["ratings"]["reviews"] = reviews_match.group(1)
            
            # Extract cooking time
            time_element = soup.find("div", class_="recipe-meta-item-body")
            if time_element:
                recipe["cooking_time"] = time_element.text.strip()
        
        # If ingredients or instructions weren't found with specific classes, try generic extraction
        if not recipe["ingredients"] or not recipe["instructions"]:
            # Extract recipe sections from raw text
            recipe_text = extract_text_with_bs4(response.text)
            extracted_data = extract_recipe_parts(recipe_text)
            
            if not recipe["ingredients"] and extracted_data["ingredients"]:
                recipe["ingredients"] = extracted_data["ingredients"]
                
            if not recipe["instructions"] and extracted_data["instructions"]:
                recipe["instructions"] = extracted_data["instructions"]
                
            if not recipe.get("cooking_time") and extracted_data["cook_time"]:
                recipe["cooking_time"] = extracted_data["cook_time"]
        
        return recipe
        
    except Exception as e:
        return {"error": f"Failed to extract recipe details: {str(e)}"}


def get_related_recipes(query: str, limit: int = 3) -> List[Dict]:
    """
    Find related recipes for a given query
    
    Args:
        query (str): Search query or main ingredient
        limit (int): Maximum number of related recipes to return
        
    Returns:
        list: List of related recipe dictionaries
    """
    # Generate alternative search queries
    related_queries = []
    
    # Remove words like "recipe" and "how to make" from query to get core ingredient
    core_query = re.sub(r'recipe|how to|make|prepare|cook', '', query, flags=re.IGNORECASE).strip()
    
    # Check if this is an Indian recipe query
    indian_cuisine_terms = ["indian", "curry", "masala", "paneer", "tikka", "biryani", 
                           "dosa", "chutney", "samosa", "naan", "roti", "chapati", 
                           "dal", "paratha", "korma", "tandoori", "idli", "sambar"]
    
    is_indian_query = any(term in query.lower() for term in indian_cuisine_terms)
    
    # If it's an Indian recipe query, generate appropriate related queries
    if is_indian_query:
        # Extract the main ingredients or dish type
        main_ingredient = core_query
        for term in indian_cuisine_terms:
            main_ingredient = main_ingredient.replace(term, "").strip()
        
        # Generate related Indian recipe queries
        if main_ingredient:
            related_queries = [
                f"{main_ingredient} curry",
                f"{main_ingredient} masala",
                f"{main_ingredient} korma",
                f"{main_ingredient} tikka",
                f"{main_ingredient} biryani",
                f"tandoori {main_ingredient}",
                f"{main_ingredient} paratha",
                f"{main_ingredient} with rice",
                f"{main_ingredient} sabzi"
            ]
        else:
            # If we couldn't extract a main ingredient, suggest popular Indian dishes
            related_queries = [
                "butter chicken",
                "chicken tikka masala",
                "vegetable biryani",
                "paneer tikka",
                "aloo gobi",
                "palak paneer",
                "chicken korma",
                "chana masala",
                "dal makhani"
            ]
    # Regular (non-Indian) recipe logic
    elif len(core_query.split()) <= 2:
        main_ingredient = core_query
        related_queries = [
            f"{main_ingredient} pasta",
            f"{main_ingredient} soup",
            f"{main_ingredient} salad",
            f"{main_ingredient} curry",
            f"grilled {main_ingredient}",
            f"roasted {main_ingredient}",
            f"{main_ingredient} stir fry",
            f"{main_ingredient} with rice",
            f"{main_ingredient} casserole"
        ]
    else:
        # Try different cuisine variations
        cuisines = ["Italian", "Mexican", "Indian", "Thai", "Mediterranean", "American", "Chinese"]
        for cuisine in cuisines[:5]:
            related_queries.append(f"{cuisine} {core_query}")
    
    # Shuffle the related queries for variety
    random.shuffle(related_queries)
    
    # Get recipes for each related query
    related_recipes = []
    
    for related_query in related_queries:
        if len(related_recipes) >= limit:
            break
            
        results = search_recipes_with_ratings(related_query, limit=1)
        if results:
            for recipe in results:
                if len(related_recipes) >= limit:
                    break
                # Avoid duplicates
                if not any(r["name"] == recipe["name"] for r in related_recipes):
                    related_recipes.append(recipe)
    
    return related_recipes